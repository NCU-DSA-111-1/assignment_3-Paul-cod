IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 1 
Wireless Ad Hoc Federated Learning:A Fully Distributed Cooperative Machine Learning 
Hideya Ochiai, Yuwei Sun, Qingzhe Jin, Nattanon Wongwiwatchai, and Hiroshi Esaki, Member, IEEE 
Abstract—Federated learning has allowed training of a global model by aggregating local models trained on local nodes. However, it still takes client-server model, which can be further distributed, fully decentralized, or even partially connected, or totally opportunistic. In this paper, we propose a wireless ad hoc federated learning (WAFL) – a fully distributed cooperative machine learning organized by the nodes physically nearby. Here, each node has a wireless interface and can communicate with each other when they are within the radio range. The nodes are expected to move with people, vehicles, or robots, producing opportunistic contacts with each other. In WAFL, each node trains a model individually with the local data it has. When a node encounter with others, they exchange their trained models, and generate new aggregated models, which are expected to be more general compared to the locally trained models on Non-IID data. For evaluation, we have prepared four static communication networks and two types of dynamic and opportunistic communication networks based on random waypoint mobility and community-structured environment, and then studied the training process of a fully connected neural network with 90% Non-IID MNIST dataset. The evaluation results indicate that WAFL allowed the convergence of model parameters among the nodes toward generalization, even with opportunistic node contact scenarios – whereas in self-training (or lonely training) case, they have diverged. This WAFL’s model generalization contributed to achieving higher accuracy 94.7- 96.2% to the testing IID dataset compared to the self-training case 84.7%. 
Impact Statement—Privacy-related legal regulations are getting severe to protect users including children against unintended use of collected personal data. Federated learning allowed global model training without collecting data from them, somewhat providing a technical solution to this problem. However, the parameter server is still operated as a top-down manner, and the users must connect to the server – otherwise they cannot obtain the service even for very simple applications. WAFL allows fully distributed machine learnings without infrastructures including the Internet but with friends who have physical contacts and trust. 
Index Terms—Ad hoc networks, Decentralized deep learning, Federated learning, Non-IID data, Peer-to-Peer systems 
I. INTRODUCTION 
CLOUD computing has allowed massive data collection from users, forming Big Data for machine learning in 2010s [1], [2]. However, because of the privacy concerns, such as (1) utilization of them other than the original intent or (2) leakage of them by cyberattacks, privacy related regulations 
This work was supported by JSPS KAKENHI Grant Number JP 22H03572. 
Hideya Ochiai, Yuwei Sun, Qingzhe Jin, Nattanon Wongwiwatchai, and Hi- roshi Esaki are with the Graduate School of Information Science and Technol- ogy, University of Tokyo, Tokyo, 113-8656, Japan (e-mail: ochiai@elab.ic.i.u- tokyo.ac.jp). 
Fig. 1.their models with each other when they encounter, and then aggregate them into new models. The aggregated models are expected to be more general, compared to the locally trained models with their own Non-IID dataset. 
are getting severe these days in order to protect especially those who are not information literate including children against unintended use of personal data [3]–[6]. 
Federated learning (FL) has emerged as one of promising technologies for this issue [7], [8]. It can train a global model by aggregating local models trained on local nodes without uploading its local data. However, from the distributed system’s perspective, FL still takes a centralized architecture, i.e., Client-Server model. This can be (1) further distributed, or (2) fully-decentralized, or even (3) partially connected, or (4) totally opportunistic, which can be said – Beyond Federated Learning. 
We propose a wireless ad hoc federated learning (WAFL) – a fully distributed cooperative machine learning organized by the nodes physically nearby. Here, each node has a wireless interface and can communicate with each other when they are within the radio range. The nodes are expected to move with people, vehicles, or robots, producing opportunistic contacts with each other. 
In WAFL, we can start the discussion from the most basic peer-to-peer federated learning as shown in Fig. 1. In this scenario, each node trains a model individually with the local data it has. When a node encounter with another node, they exchange their local models with each other through the ad hoc communication channel. Then, the node aggregates the models into a new model, which is expected to be more general compared to the local trained models. With an adjustment process of the new model with the local training data, they repeat this process during they are in contact. 
This paragraph will include the Associate Editor who handled your paper. 
As WAFL does not collect data from users, the distributions 
The most basic peer-to-peer federated learning. They exchange 

arXiv:2205.11779v1 [cs.LG] 24 May 2022 
2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 

Fig. 2. Model aggregation with encounter nodes in wireless ad hoc federated learning (WAFL). The nodes exchange and aggregate their models among the nodes encountered in ad hoc manner. The initial models are trained too specific to their local Non-IID data, but after the long run, many contacts allow the mixture of locally trained models and to develop more generalized models. 
of the data on individual nodes are not the same; e.g., a user has a larger portion of photos of hamburger, but another user has a larger portions of dumpling based on their preferences or circumstances. This is a well-known problem of conventional federated learning as “user data is not independent and identi- cally distributed (Non-IID)” [9]. The challenge is to develop a general model which does not over-fit into a specific user data on the fully distributed, or partially-connected environment. 
In this paper, we introduce several node mobility patterns for evaluation from static cases to dynamic cases. In static cases, nodes do not move and organize static networks. We consider the topologies of line, tree, ringstar, and fullmesh in this study. In dynamic cases, nodes move around and sometimes make opportunistic contacts with each other. We consider several movement patterns based on random waypoint (RWP) mobility [10], and community structured environment (CSE) mobility [11]. 
We use Non-IID dataset for training, and IID dataset for test- ing for evaluating the generalization of the developed models. We create Non-IID dataset from MNIST dataset based on the study of [12]. We use a fully-connected neural network for the classification of data samples intended for understanding the basic nature of WAFL learning scheme including the behavior of model parameters. 
We admit that a fully-connected neural network with MNIST dataset is rather simple configurations if we consider the today’s evolution of machine learning researches. Here, we emphasize that our focus or main contribution is the proposal of wireless ad hoc federated learning, which can be further applied into convolutional neural networks(CNN) [13], graph neural networks(GNN) [14], long short-term memory(LSTM) 
[15], transfer learning schemes, Autoencoder [16], generative adversarial network [17], word to vector [18], reinforcement learning [19], and so on. We focus on the analysis of model aggregations with simple layer settings with baseline dataset on varieties of mobility scenarios. Further applications are open problems and beyond the scope of this paper. 
This paper is organized as follows. Section II describes the related work. In section III, we propose wireless ad hoc federated learning. In section IV, we provide performance eval- uation and model analysis. Section V provides the discussion, and we finally conclude this paper in section VI. 
II. RELATED WORK 
Wireless ad hoc federated learning (WAFL) is associated with wireless ad hoc network (WANET) [20], which is often discussed as a family of peer-to-peer (P2P) [21] network in that it is server-less. However, P2P is usually discussed as an overlay network of the Internet infrastructure, which is constructed by the nodes joined on the Internet, finding with each other, and constructing another network over the Internet. WANET is infrastructure-less. Even without infrastructure, it can construct a network based on opportunistic physical contacts. Mobile ad hoc network (MANET) [22], vehicular ad hoc network (VANET) [23], delay/disruption tolerant network (DTN) [24] are the family. 
According to a comprehensive survey of federated learning [25], our approach could be categorized into decentralized federated learning (DFL). BrainTorrent [26] was proposed as a P2P decentralized federated learning where all the nodes on their P2P network are possible to communicate with each 
H. OCHIAI et al.: WIRELESS AD HOC FEDERATED LEARNING: A FULLY DISTRIBUTED COOPERATIVE MACHINE LEARNING 3 
other, and a node selects a target node for sharing and aggre- gating their models. In the context of P2P, there were several attempts of applying Blockchain to add P2P characteristics to the federated learning. However, in spite of P2P network, blockchain-based federated learning basically has an idea of “global model” for downloading, uploading, and aggregating [27]–[30]. Recently, swarm learning [31] has been also pro- posed on a blockchain network for the application of machine learning to medical data. These systems are built on a peer- to-peer system, but the service is not basically decentralized. The fully decentralized federated learning should not have the idea of global model. 
Gossip-based federated learning (GFL) [32], [33] could be said as another form of P2P federated learning. In GFL, the nodes can talk with neighbors, and this allows the propagation of the models. This is inspired from Gossip-based protocol [34] – another form of ad hoc networks which relies on flooding-based or epidemic routing [35]. 
Federated learning is now also combined with unmanned aerial vehicles (UAV) [36], where the nodes have to commu- nicate over wireless channels. UAV’s network is sometimes discussed with flying ad hoc network (FANET) [37]. In this application, they currently assume the server-role UAV, which is the form of centralized federated learning [38], [39]. Other studies focus on the efficiency of power consumption [40], [41] rather than the architecture of federated learning. 
WAFL, our proposal, is fully autonomous, distributed, and tolerant for network partitioning or opportunistic contacts. WAFL does not have the idea of centralized global model, but still allows to develop a generalized model at each node by exchanging trained models from local Non-IID dataset through encounter-based ad hoc communication channels. 
III. WIRELESS AD HOC FEDERATED LEARNING 
In wireless ad hoc federated learning (WAFL), each node has a wireless communication interface, and can interact with the nodes when they are within the radio range. A node has their own data for training, but it will not be exchanged with others because of the privacy issues. Instead, they can exchange the parameters of the trained models with others, which can be aggregated to make the models more general. 
Here, we assume that the data owned by a node is not in- dependent and identically distributed (Non-IID). For example, Node 1 in Fig. 1 and 2 has larger portion of label 1 data, whereas Node 2 has larger portion of label 2 data. If they train their model parameters simply based on their own local dataset, it will not become a generalized model, giving a poor performance on the accuracy to a general (i.e., an IID) testing dataset. Thus, in WAFL, we allow each node to exchange the trained model with the encountered nodes, and to aggregate the models. The developed model in this way will provide better performance because it originates in both larger portions of label 1 and 2 data samples. However, it is still oriented for 1 and 2 data. In WAFL scenario, as Fig. 2 shows, the nodes move around and make opportunistic contacts among them. After the long run, with enough contacts based on node mobility, we expect that the developed models will provide further better performance to an IID testing dataset. 
Let (X(n), Y (n)) ∈ D(n) be a dataset owned by node n ∈ {1, 2, . . . , N}, and let θ(n) be the model parameters of node n. For x ∈ X(n), we present f(x;θ(n)) gives the prediction yˆ :i.e.,yˆ=f(x;θ(n)). 
For each mini-batch, denoted by D(n) ⊂ D(n), we calculate B 
the loss byJ(θ,DB ) = (n) l(y,f(x;θ)). (1) 
(n) 1 X #(DB ) (x,y)∈D(n) 
￼
B 
Here, # denotes the number of elements in the set, and l denotes the loss function. 
Conventionally, we calculate the next model parameters by 
(n) (n) (n) (n)θi+1 =θi −η∇J(θi ,DB ). (2) 
Here, we denote i by total mini-batch iterations. If D(n) is divided into m mini-batches in an epoch, with given epoch e and jth mini-batch, i = m×e+j. Please note that η is learning rate. 
A. Model Aggregation with Encounter Nodes 
In WAFL, when a node encounter others, they exchange their own models with each other, and aggregate them by themselves. In other words, a node sends its own model to the neighbors, and at the same time, it receives other models from the neighbors. Then, it aggregates them into its own model. And, repeats this process every time during they are in contact. 
Let nbr(n) ⊂ {1,2,...,N} \ {n} be a set of neighbors of node n ∈ {1,2,...,N}. Please note that nbr(n) does not include itself n ∈/ nbr(n). In this study, we assume that the communication link is symmetric; ∀k, n: k ∈ nbr(n) ⇔ n ∈ nbr(k). 
We define model aggregation in WAFL as follows. 
θ′(n) = θ(n) + λ me me 
P (k) (n)k∈nbr(n) (θ − θme). (3) 
#(nbr(n)) + 1 (n) 
￼
Here, λ is a coefficient. θme, i.e., the model parameters of node n at epoch e, is aggregated with the received model parameters from neighbors k ∈ nbr(n) with the coefficient λ. Here, the epoch number is not explicitly provided for neighbor model parameters θ(k), because it is not necessary to be the same epoch. 
Then, the node adjust the model parameters with the local dataset, i.e., for each {0, 1, . . . , m − 1} mini-batch of D(n): 
′(n) θme+1 θ′(n) 
me+2 
θ′(n) me+m 
′(n) ′(n) (n) θme+0 − η∇J(θme+0, DB0 ), 
= 
= 
. 
= θ′(n)me+m−1 me+m−1 
θ′(n) − η∇J(θ′(n) me+1 me+1 
, D(n)), B1 
−η∇J(θ′(n) 
,D(n) ). (4) Bm−1 
This adjustment process should be carried out only if #(nbr(n)) > 0, where the mixture of model parameters by Eqn. 3 is effective. If #(nbr(n)) = 0, this minibatch-based adjustment process should be skipped because it causes over- fitting to the local dataset. 
4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 

Fig. 3. Trajectories of model parameters θ on the loss fields in the parameter space. In this example, Node n and k encounter with each other at epoch=0 and 1, and then leave at epoch=2. Instead, node n encounters with node h at epoch=2 and 3. The numbers shown in the figure indicate the epochs described in this statement. After the pre-training process, the initial (i.e., epoch=0) model parameters θ(n), θ(k), and θ(h) are located near the argmins of J(θ,D(n)), J(θ,D(k)), and J(θ,D(h)) respectively. After the process of model aggregation (shown by red arrows) with local mini-batches (shown by blue arrows), the model parameters are shifted toward the argmin of J(θ, D(n) ∪ D(k) ∪ D(h)) – the loss with virtually merged data of node n, k, and h. The real model parameter dimensions are very large. Please note that this 2-dimensional illustration is a conceptual presentation of the process. 
Finally, we get the model parameters of the next epoch e + 1 by : 
This means that the process of WAFL is to find better θ(n) that minimizes the loss on the virtually merged dataset. 
Please note that this self-training should not be carried out after starting model exchange phase. It loses learnt parameters, i.e., over-fits to the local data, especially when it runs many self-training epochs. 
C. Insight into Model Parameters 
If #(nbr(n)) = 1, Eqn. 3 can be simplified into the following formula with the sole neighbor k: 
′(n) λ (n) λ (k) θme=1−2θme+2θ . (8) 
(n)This means that the model parameters θme is shifted toward 
θ(k).Ifλ=2,itwillbereplacedwithθ(k).Ifλ=1,itis 
B. Pre-Training for WAFL Environment 
θ(n) m(e+1) 
= θ′(n) me+m 
. 
(5) 
In our study, we found that before actually starting model exchange, each node should train their model parameters by itself with their own local data. This initial self-training process allows rapid increase of accuracy in WAFL’s model aggregation phase. 
This self-training can perform the following calculations independently at each node n, 
(n) 
data. Thus, for the virtually merged dataset, we will observe: 
(n) θ0 
≈ argminJ(θ,D θ 
).These model parameters are optimized for local training 
NNJ(θ(n), [ D(u)) ≫ min J(θ, [ D(u)). (7) 
λ will be set to 1 or less: i.e., 0<λ≤1.Fig. 3 explains why this model aggregation is effective 
for finding optimal model parameters over virtually merged dataset. 
As described, the aggregation of the model parameters with Eqn. 3 or Eqn. 8 makes θ′(n) shifted toward minimizing 
0θu=1 u=1 
(6) 
(n) (k) shifted at the center position between θme and θ 
. Practically, 
H. OCHIAI et al.: WIRELESS AD HOC FEDERATED LEARNING: A FULLY DISTRIBUTED COOPERATIVE MACHINE LEARNING 5 
TABLE ITHE CONFIGURATION OF NON-IID MNIST DATA SAMPLES. 90% OF NODE n’S DATA IS COMPOSED OF LABEL n’S SAMPLES, AND THE OTHER 10% IS UNIFORMLY COMPOSED OF THE OTHER LABEL’S SAMPLES WITHOUT ANY OVERLAPS AMONG THE NODES. 
Node L0 L1 L2 L3 L4 L5 L6 L7 L8 L9 Summary 
	.	0  5341 76 64 76 61 57 57 61 74 50 5917 
	.	1  79 6078 67 52 57 59 58 79 59 68 6656 
	.	2  58 67 5374 80 64 68 87 73 57 61 5989 
	.	3  68 74 73 5537 51 56 72 65 67 77 6140 
	.	4  73 67 80 67 5301 59 53 69 70 64 5903 
	.	5  60 66 57 74 68 4896 61 59 66 69 5476 
	.	6  52 78 53 56 58 66 5312 56 65 65 5861 
	.	7  67 90 66 74 55 61 65 5683 63 77 6301 
	.	8  59 80 59 54 63 48 88 67 5268 57 5843 
	.	9  66666561645165536253615914 
￼ ￼ ￼
Summary 5923 6742 5958 6131 5842 
J(θ(n),D(k)) (denoted by red arrow in the figure). With the mini-batches, i.e., Eqn. 4, it shifts toward minimizing J(θ(n),D(n)) (denoted by blue arrow in the figure). After several epochs, it will find better parameters for reducing J(θ(n), D(n)∪D(k)), without actually exchanging the datasets: i.e., D(n) and D(k). 
In WAFL, after finding the better parameters in this way, these two nodes will be separated, and in the next phase, encounter other nodes. Let’s assume that n encounters h, here. Then, the parameters will be shifted to another direction – toward minimizing J(θ(n),D(h)), but this does not mean that it forgets the learnt parameters for J(θ(n), D(n) ∪ D(k)), especially when 
∇J(θ(n), D(h)) ⊥ ∇J(θ(n), D(n) ∪ D(k)). (9) 
This is because the parameter space is multi-dimensional. After the long run with many encounters with various nodes, the model parameters will be tuned for the virtually merged 
5421 5918 6265 5851 5949 60000 
RWP mobility [42] is often used in wireless ad hoc network simulations. The nodes walk around the given area with posing some duration at temporal locations. This mobility model allows a node to contact with any other nodes in the area. This mobility model greatly fits to the movement in a shopping mall or an amusement park. 
CSE mobility [11] simulates node movement within a given structure of communities. CSE is based on the idea that there are several communities in the real society where people belong to, and that they usually meet the people who belong to the same community. CSE assumes that a node belongs to several communities and moves from one community to another. Here, the community is a physical rendezvous point. If two nodes are in the same community at the same time, they can communicate with each other. 
In case of RWP mobility, depending on the size of the node movable areas, we generated three categories of mobility patterns: i.e., rwp0500, rwp1000, and rwp2000. Here, the name indicates the size of the movable area, for example, rwp0500 was generated by simulating node movements in 500[m]×500[m] square. We assumed 100[m] for radio range, and 10[epoch] for posing time. We have chosen the travel speed uniformly randomly from the interval of [3.0, 7.0] [m/epoch]. 
In case of CSE mobility, depending on the number of communities a node belongs to, we generated three categories of mobility patterns: i.e., cse02, cse04, and cse08. The number indicates the number of communities a node belongs to, for example, cse02 means that a node belongs to two communities in the world of communities. In this evaluation, we have assumed 10 communities as the world of communities, transit time from a community to another as 10[epoch], and transit starting probability at a community as 5[%]. 
Fig. 4 shows topology examples of node-to-node contacts generated in this way, and used in our evaluation. The solid lines indicate static connections – the nodes were always connected and could communicate all the time. The dashed lines indicate opportunistic connections – the nodes have seen temporal connections and could communicate while the connections were alive. 
For the dynamic cases, i.e., RWPs and CSEs, we have generated three mobility patterns for each individual category with different random seeds. For example, as Fig. 4 shows, cse02 category has three network topology: cse02-1, cse02-2, 
￼
data, 
Nθ(ˆn) ≈ argmin J(θ(n), [ D(u)). (10) 
θ(n) u=1 IV. EVALUATION 
We have carried out performance evaluation on several static and dynamic communication networks with simulating the mobility of the nodes. This evaluation includes the analysis of developed models by WAFL algorithm. 
A. Experiment Setting 
1) Non-IID Dataset: In this evaluation, we have configured 10 training nodes (n = 0,1,...,9) to have 90% Non-IID MNIST dataset; i.e., 90% of node n’s data is composed of label n’s samples, and the other 10% is uniformly composed of the other label’s samples without any overlaps among the nodes. Table I describes the detail of data sample distributions. The distribution parameter 90% is more severe than the settings given by [12], which was 80%. 
2) Node Mobility: We have generated node-to-node contact patterns of static and dynamic networks as Fig. 4. We pre- pared four static networks (1) static line, (2) static tree, (3) static ringstar, and (4) static dense, and two types of dynamic networks based on (1) random way point (RWP) and (2) community structured environment (CSE). 
6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 

Fig. 4. Examples of node-to-node contact pattern used for the evaluation. The solid lines indicate static connections, and the dashed lines indicate opportunistic connections. We have prepared four static networks, and two types of dynamic networks – (1) random way point mobility (RWP) and (2) community structured environment (CSE). 
and cse02-3.In this evaluation, we have configured so that a time unit 
corresponds to a epoch. A single training epoch is assumed to be completed within a single time unit, e.g., in a minute or 10 seconds. We have carried out simulations of each case up to 5000 epochs. 
3) Machine Learning Model: As for the target model in this study, we have used a 2-layer fully connected neural network. The input of a MNIST data sample (28×28) is first flatten into 784 elements, and then the first fully connected layer (fc1) with the output of 128 elements comes. Then, the 128 elements go through ReLU function, and the second fully connected layer (fc2) gives the output of 10 elements. This model predicts the label of the sample by finding the index of the maximum. We have given random value as the initial parameters before the pre-training. After 50 epochs for pre-training, we applied WAFL. We used CrossEntropy as a loss function, and Adam as an optimizer. We have used 32 as the batch size and evaluated with 0.001 and 0.0001 as the learning rate η. We have also studied with coefficient λ=1.0 and 0.1. 
We admit that this model setting is the very basic of the today’s evolution of machine learning researches, but we emphasize again that our evaluation focus is on the study of WAFL on the varieties of node-to-node contact patterns. 
4) Self-Training and Federated Learning: To compare the performance of WAFL, we have also studied the developed models by self-training and federated learning. 
In self-training, in this paper, we assumed that nodes are closed and make training lonely. They do not make any communications including model exchange with others. The nodes make continuous training with the local Non-IID data 
they have individually. In this training scheme, they don’t have coefficient λ, but it has learning rate η. 
In federated learning, we assumed a virtual parameter server that collects local models from all the nodes, aggregates them with the global model it has, and redistributes to the nodes. In this training scheme, they have coefficient λ and learning rate η. 
In both training methods, we have initialized model param- eters at random, and carried out 50 epoch pre-training as the other cases do. Other settings (i.e., data distributions, model structure, loss functions, Adam optimizer) were also the same. 
B. Performance Overview 
Table II shows the performance of WAFL on the test data on different contact patterns, coefficient λ and learning rate η. For this analysis, we have taken the models of the last 100 epochs, i.e., from epoch 4901 to 5000, and calculated averaged accuracy, precision, recall and F1-score with the test MNIST dataset in the following manner. As this is a multi-class classification task, we have first calculated those metrics by each node, each class, and each epoch, which we call micro- accuracy, micro-precision, micro-recall and micro-F1-score. Then, we got 10×10×100 = 10000 micro-accuracies for each static case. We did the same calculations to the other metrics, i.e., precision, recall, and F1-score. As for the dynamic cases, we got 10×10×100×3 = 30000 micro-accuracies because we have three contact patterns for each case. The discussion of this approach - the arithmetic mean over individual micro- metrics are provided in [43]. Here, the error values on the table indicates the standard deviation of the micro-accuracies, 
precisions, recalls, and F1-scores. 
H. OCHIAI et al.: WIRELESS AD HOC FEDERATED LEARNING: A FULLY DISTRIBUTED COOPERATIVE MACHINE LEARNING 7 
TABLE IIPERFORMANCE EVALUATION OF WAFL ON DIFFERENT CONTACT PATTERNS WITH COEFFICIENT λ AND LEARNING RATE η. COMPARED TO SELF-TRAIN, WAFL HAS ACHIEVED GOOD PERFORMANCE ALMOST NEARLY FEDERATED LEARNING. IN STATIC CONTACT CASES INCLUDING FEDERATED LEARNING, λ = 1.0 PROVIDED GOOD PERFORMANCE EXCEPT STATIC-DENSE, WHEREAS IN DYNAMIC CONTACT CASES, λ = 0.1 PROVIDED BETTER PERFORMANCE. THESE RESULTS INDICATE THAT IN WAFL, IF THEY ARE DYNAMIC, THE COEFFICIENT FOR MODEL AGGREGATION SHOULD BE SMALLER THAN STATIC CASES. 
￼ ￼
Mobility (Experiment Settings) static-line (λ=1.0, η=0.001) static-line (λ=1.0, η=0.0001) static-line (λ=0.1, η=0.001) static-line (λ=0.1, η=0.0001) static-tree (λ=1.0, η=0.001)static-tree (λ=1.0, η=0.0001) static-tree (λ=0.1, η=0.001)static-tree (λ=0.1, η=0.0001) static-ringstar (λ=1.0, η=0.001) static-ringstar (λ=1.0, η=0.0001) static-ringstar (λ=0.1, η=0.001) static-ringstar (λ=0.1, η=0.0001) static-dense (λ=1.0, η=0.001) static-dense (λ=1.0, η=0.0001) static-dense (λ=0.1, η=0.001) static-dense (λ=0.1, η=0.0001) dynamic-rwp-0500 (λ=1.0, η=0.001) dynamic-rwp-0500 (λ=1.0, η=0.0001) dynamic-rwp-0500 (λ=0.1, η=0.001) dynamic-rwp-0500 (λ=0.1, η=0.0001) dynamic-rwp-1000 (λ=1.0, η=0.001) dynamic-rwp-1000 (λ=1.0, η=0.0001) dynamic-rwp-1000 (λ=0.1, η=0.001) dynamic-rwp-1000 (λ=0.1, η=0.0001) dynamic-rwp-2000 (λ=1.0, η=0.001) dynamic-rwp-2000 (λ=1.0, η=0.0001) dynamic-rwp-2000 (λ=0.1, η=0.001) dynamic-rwp-2000 (λ=0.1, η=0.0001) dynamic-cse-02 (λ=1.0, η=0.001) dynamic-cse-02 (λ=1.0, η=0.0001) dynamic-cse-02 (λ=0.1, η=0.001) dynamic-cse-02 (λ=0.1, η=0.0001) dynamic-cse-04 (λ=1.0, η=0.001) dynamic-cse-04 (λ=1.0, η=0.0001) dynamic-cse-04 (λ=0.1, η=0.001) dynamic-cse-04 (λ=0.1, η=0.0001) dynamic-cse-08 (λ=1.0, η=0.001) dynamic-cse-08 (λ=1.0, η=0.0001) dynamic-cse-08 (λ=0.1, η=0.001) dynamic-cse-08 (λ=0.1, η=0.0001) federated (λ=1.0, η=0.001) 
federated (λ=1.0, η=0.0001) federated (λ=0.1, η=0.001) federated (λ=0.1, η=0.0001) self-train (η=0.001) self-train (η=0.0001) 
Accuracy[%] 96.195±0.940 86.284±3.848 95.157±0.888 91.834±0.943 96.099±1.113 86.055±2.927 95.129±1.081 91.749±1.172 95.773±1.125 85.999±3.131 95.471±0.794 92.197±1.219 94.976±1.582 86.325±2.975 95.691±0.975 92.404±1.244 94.425±1.268 90.837±1.469 95.585±0.754 92.389±1.068 92.320±1.753 89.218±2.387 95.212±0.722 92.543±1.054 86.785±3.878 84.047±4.302 93.263±0.824 90.392±1.293 89.185±3.150 84.336±3.390 94.665±1.030 92.094±1.048 92.887±1.348 89.268±2.428 95.329±0.791 92.713±0.972 92.690±1.356 89.584±1.995 95.186±1.646 92.494±0.990 96.756±0.928 85.882±4.392 94.261±1.681 91.204±1.830 84.663±1.285 82.160±1.543 
Precision[%] 96.310±3.055 90.137±12.399 95.394±4.257 93.040±8.345 96.220±3.306 89.830±12.441 95.414±4.544 93.036±8.518 95.901±3.398 89.894±12.645 95.686±4.230 93.369±8.306 95.344±4.809 89.990±12.311 95.860±3.754 93.487±7.933 94.981±5.975 92.538±9.490 95.779±3.986 93.541±8.225 93.528±8.294 91.376±10.397 95.545±4.964 93.772±8.454 89.353±11.040 87.492±12.368 94.097±7.212 92.199±9.769 90.918±9.708 87.872±12.312 95.102±5.493 93.417±8.712 93.910±7.749 91.618±10.639 95.643±4.818 93.876±8.296 93.807±8.067 91.786±10.433 95.602±5.257 93.731±8.496 96.806±2.518 90.577±13.357 95.073±7.074 93.220±10.223 86.363±9.356 84.731±11.003 
Recall[%] 96.159±2.659 86.161±10.005 95.111±3.309 91.755±5.178 96.059±2.851 85.957±10.119 95.071±3.447 91.651±5.564 95.728±2.786 85.867±9.844 95.422±2.825 92.101±5.302 94.939±4.123 86.216±8.957 95.650±2.612 92.321±5.131 94.379±3.912 90.760±5.972 95.547±2.729 92.314±4.999 92.253±5.461 89.124±7.394 95.169±3.120 92.465±5.035 86.656±9.113 83.854±10.904 93.201±4.384 90.286±6.419 89.088±7.546 84.175±10.001 94.604±3.672 92.010±5.267 92.834±4.886 89.170±7.426 95.289±3.144 92.634±4.886 92.633±4.943 89.497±6.960 95.147±3.829 92.415±4.947 96.724±2.433 85.760±10.897 94.217±4.816 91.117±6.680 84.445±9.319 81.922±10.684 
F1-score[%] 96.178±1.912 86.944±8.419 95.144±2.439 91.992±4.561 96.075±2.169 86.672±8.418 95.120±2.659 91.915±4.887 95.751±2.191 86.679±8.454 95.460±2.331 92.342±4.786 94.996±3.247 86.995±7.791 95.682±2.233 92.536±4.470 94.483±3.345 91.109±5.424 95.577±2.160 92.547±4.560 92.480±4.738 89.546±6.432 95.226±2.620 92.721±4.715 87.129±7.410 84.511±9.046 93.361±3.862 90.656±5.729 89.397±6.171 84.874±8.203 94.685±3.000 92.281±4.859 93.027±4.320 89.648±6.532 95.343±2.607 92.874±4.579 92.854±4.524 89.947±6.232 95.215±3.312 92.673±4.683 96.728±1.876 86.748±9.392 94.367±4.355 91.534±6.213 84.663±6.181 82.281±7.253 
￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ Fig. 5.\training as the federated learning did. We could observe slower training speeds on dynamic cases. This is because nodes skipped training while they were not in contact. 
Accuracy trend on (a) static network, (b) RWP mobility, and (c) CSE mobility with the best hyper parameters (Table II). Static network allowed fast 
8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 

Fig. 6. Reduction of confusions in static and dynamic contact examples. These results show the confusion matrices of node 9 with the test data which has true labels uniformly randomly. In the beginning at epoch 1, the model had confusions especially for label 4 and 8. These confusions were decreased gradually, and the model could improve overall classification accuracy finally at epoch 5000. 
From these results, we have observed that self-train could achieve only 84.7 [%] accuracy whereas WAFL overall (ex- cluding dynamic-rwp-2000) could achieve around 94.7-96.2 [%], which is close to the conventional federated learning case 96.8[%], improving 10.0-11.5[%] from the self-train (i.e., lonely) case. The accuracy of dynamic-rwp-2000 is low because of the sparseness and potential less contacts among the nodes compared to the other cases, which we describe in the next subsection. 
Regarding coefficient λ and learning rate η, we observed that except static-dense, static networks including federated learning provided better accuracy with λ = 1.0 rather than λ = 0.1, and that dynamic networks provided better with λ = 0.1. Lower learning rates did not contribute to higher accuracy. 
These results indicate that especially in dynamic cases where nodes were not always connected with each other, coefficient λ = 1.0 experienced over-fit to some recently encountered nodes, but with λ = 0.1 model parameters learnt in the past could remain in the model. 
We also observed in static cases that the sparser the network became the higher accuracy it could get. These results indicate that if model parameter exchange is limited to smaller number of connected nodes in dense network scenarios, it might provide better accuracy. 
C. Trend of Accuracy 
Fig 5 shows the trend of the test accuracy with the best hyper parameters shown in Table II. In static networks, as nodes were always connected and could always exchange model parameters among them, they rapidly trained the model with increasing the accuracy just as the same as the conven- tional federated learning. In dynamic networks, the training speed was low compared to the static cases and the federated learning. This is simply because in dynamic networks, during the nodes were alone, they did not make any training process. Especially, the training speeds on RWP mobility cases were rwp0500 > rwp1000 > rwp2000. This corresponds to the density of nodes in the mobility area. Higher density allowed 
H. OCHIAI et al.: WIRELESS AD HOC FEDERATED LEARNING: A FULLY DISTRIBUTED COOPERATIVE MACHINE LEARNING 9 

Fig. 7. The trends of model convergence errors. The static cases and the conventional federated learning provided the similar trends including the values of the error. The dynamic cases also allowed gradual error reductions, indicating model aggregation in WAFL has enabled cooperative learning. If not cooperated, i.e., self-training case, the convergence errors were always increasing. 
faster training. This shows that rwp2000 case requires longer time to have more contacts and to obtain enough accuracy. 
D. Reduction of Confusions 
Fig. 6 shows some examples of confusion reductions. These examples are the confusion matrices of the model of node 9 for static line, dynamic rwp0500, and dynamic cse02 cases. 
At epoch=1, i.e., after the single round of WAFL algorithm with encountered nodes, we can observe that the models had wrong predictions regarding some data samples as label 9’s samples. For example, about 37-39% of label 4’s samples were predicted as 9, and about 28-31% of label 7’s samples were as 9. This is because just after the pre-self-training process 
with only a single WAFL epoch, it was over-fitted to the local training data at node 9, which has much larger portions of label 9’s samples compared to the other samples. 
We can also observe that label 2’s samples were predicted as label 3 (4%) or 8 (7%), label 5 as 3 (4-7%) or 8 (6-10%), and label 6 as 5 (1-5%). These confusions were made probably because of the much less data samples regarding label 0-8’s data for training. 
As the training proceeds, at epoch=200 and epoch=5000, we can observe that they could gradually reduce confusions, achieving better performance in general. 
10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 

Fig. 8. UMAP presentation of model parameters (fc2.weight) by node from epoch=1 to epoch=5000. The color indicates the node number. 
E. Model Convergence 
We have investigated the convergences of model parameters. In this evaluation, we consider the distances among model parameter vectors of the ten nodes. Let us consider the ten model parameter vectors θ(n)(n = 0, . . . , 9), and calculate distances from the mean of these parameter vectors. Here, θ(n) can be one of fc1.weight, fc1.bias, fc2.weight, and fc2.bias. Then, we calculate the averaged distance as the metric of model convergence errors. 
More formally, at a specific epoch, we first calculate the mean of model parameter vectors by node, 
Nθ ̄= 1Xθ(n). (11) 
N n=1Then, we calculate the averaged distance from the mean 
H. OCHIAI et al.: WIRELESS AD HOC FEDERATED LEARNING: A FULLY DISTRIBUTED COOPERATIVE MACHINE LEARNING 11 
parameter θ ̄ as convergence error E: 
1 XN 
coefficient λ. If λ = 0.1 instead of λ = 1.0, the parameters of the models did move around without concentrating into a specific location at each epoch. 
In RWP cases, rwp0500 has more visible plots than rwp1000 and rwp2000. This came from the differences of opportunities of node-to-node contacts. As the nodes were denser, they could get higher chances of encounter with each other, and the model parameters had higher chances of changing. 
As for CSEs, there were no clear differences among them. But cse04 and cse08 seems to have more lines instead of dots. This indicates that they had more chances of long-time contacts among the nodes. 
As for the self-training case, the model parameters of each node independently moved around drawing multiple broken lines. 
V. DISCUSSION 
In this paper, we have proposed WAFL, and shown their ba- sic characteristics on a simple fully-connected neural network with the basic MNIST dataset rather than today’s advanced neural networks and complex datasets. This is because our focus was on the analysis of model aggregations with simple layer settings with baseline dataset on varieties of mobility scenarios. 
WAFL algorithm itself can be also applied into other kinds of deep learning models such as CNN [13], GNN [14], LSTM [15], Autoencoder [16], GAN [17], Word2vec [18], A3C [19], including more complex and heavy deep learning models such as SSD [45], BERT [46], and MDETR [47]. Such further applications of WAFL and analysis are open problems. 
Actually, simple neural networks are very practical on today’s wireless ad hoc scenarios where nodes are operated with battery-powered personal computers or smartphones. 
The size of the models we used in this study were around 400k bytes, which can be easily transferred even with Blue- tooth low energy (BLE) communication. Please note that BLE itself is already used in COVID-19 contact tracing systems [48] for exchanging physical contact information in a privacy preserving manner. 
Other practical use cases of WAFL may be realized in the context of transfer learning with very deep neural networks such as VGG [49] and ResNet [50]. Transfer learning uses pre- trained models, and only around the output layers are expected to be trained. In WAFL’s point of view, the nodes exchange such small number of trainable parameters. This could be acceptable in today’s wireless ad hoc scenarios. 
Security is another important topic. Security itself is a common issue in autonomous and cooperative distributed systems, especially when it comes to open systems. In wireless ad hoc networking, a friend mechanism [51] can be helpful for acquiring the basic security. Anomaly detection, adversarial detection, or trust mechanisms should be studied as [52], [53] did. 
VI. CONCLUSION 
In this paper, we have proposed a wireless ad hoc federated learning (WAFL) – a fully distributed cooperative machine 
Distance(θ(n),θ ̄).Here, the distance between two model parameters θ1,θ2 is 
E = Ndefined by the following formula: 
vu # θDistance(θ1, θ2) = 1 utX(θ1[i] − θ2[i])2. 
Fig. 7 shows the trend of model convergence errors cal- culated in this way. We have chosen the models developed with the best λ and η configurations studied in Table II. The static cases and the conventional federated learning provided the similar trends including the values of the error. Here, we have observed that static dense provided smoother patterns. This is probably because static dence’s λ was 0.1 whereas other’s λ was 1.0. The conventional federated learning has shown small error even from the epoch=1. This is because of the nature of aggregating into a single global model. 
As for the self-training case, the errors were always increas- ing. This indicates that long-term lonely learning diverges its model parameters. 
The dynamic cases also allowed gradual error reductions although there were some temporal error increases. This indi- cates that model aggregation in WAFL has enabled cooperative learning. If not, the model parameters will diverge as the self- training case. 
F. Model Visualization 
In order to investigate more into the model parameters, we have visualized them as Fig. 8. As model parameters are the elements of a multi-dimensional vector, we have used uniform manifold approximation and projection (UMAP) [44] for mapping them to a 2-dimension space. In this visualization, each epoch has ten vectors associated with the ten nodes. We have assigned different colors for those ten nodes from 0 to 9. We have studied all of fc1.weight, fc1.bias, fc2.weight and fc2.bias from epoch=1 to epoch=5000, but we present only fc2.weight case in the figure, because the results were similar. Please note that fc2.weight has 1280 parameters, i.e., this projection was made from the 1280-dimension space to a 2-dimension space. The configurations of λ and η correspond to the best cases shown in Table II. 
From these results, we can observe that the conventional federated learning has shown clear lines, meaning that different nodes are plotted at the same location and steadily moving in the space. The lines are sometimes broken, probably because of the sudden changes made by the optimizer. These breaks correspond to the jumps of errors in Fig. 7. 
We can also observe that static ringstar, static tree, and static line have shown similar patterns. However, especially static line case, nodes were plotted not on the exact same locations but nearby. The relationships of the locations corre- sponded to the network topology of static line (Fig. 4). 
As for static dense, it was very different from other static cases. This is most probably because of the differences of 
n=1 
(12) 
￼
#θ i=1 
(13) 
12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 00, NO. 0, MONTH 2020 
learning organized by the nodes physically nearby. In WAFL, each node trains a model individually with the local data it has. When a node encounters with other nodes, they exchange their models and generate new aggregated models. The gen- erated models in this way were expected to be more general compared to the self-trained or lonely-trained cases. 
In order to study the basic characteristics of WAFL, we have prepared four static-node networks and two types of dynamic and opportunistic contact networks, i.e., RWP and CSE, and carried out performance evaluation on 90% Non-IID MNIST dataset with a 2-layer fully connected neural network. 
We have confirmed that with WAFL, the parameters of individual local models are converged even with opportunistic contact cases, whereas in self-training (or lonely-training) case, they have diverged. We have also confirmed that the trained models in WAFL got generalized, achieving higher accuracy 94.7-96.2% to the testing IID dataset compared to the self-training case 84.7%. 
REFERENCES 
	.	[1]  Ibrahim Abaker Targio Hashem, Ibrar Yaqoob, Nor Badrul Anuar, Salimah Mokhtar, Abdullah Gani, and Samee Ullah Khan. The rise of “Big Data” on cloud computing: Review and open research issues. Information systems, 47:98–115, 2015. 
	.	[2]  Alexandra L’heureux, Katarina Grolinger, Hany F Elyamany, and Miriam AM Capretz. Machine learning with Big Data: Challenges and approaches. IEEE Access, 5:7776–7797, 2017. 
	.	[3]  Stefan Hessel and Andreas Rebmann. Regulation of Internet-of-Things cybersecurity in Europe and Germany as exemplified by devices for children. International Cybersecurity Law Review, 1(1):27–37, 2020. 
	.	[4]  Paul Voigt and Axel Von dem Bussche. The EU general data protection regulation (GDPR). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10(3152676):10–5555, 2017. 
[5] Zhifeng Xiao and Yang Xiao. Security and privacy in cloud computing. IEEE communications surveys & tutorials, 15(2):843–859, 2012. 
[6] SianiPearson.Privacy,securityandtrustincloudcomputing.InPrivacy and security for cloud computing, pages 3–42. Springer, 2013. 
	.	[7]  Jakub Konecˇny`, H Brendan McMahan, Felix X Yu, Peter Richta ́rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016. 
	.	[8]  Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020. 
	.	[9]  Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018. 
	.	[10]  Christian Bettstetter, Hannes Hartenstein, and Xavier Pe ́rez-Costa. Stochastic properties of the random waypoint mobility model. Wireless networks, 10(5):555–567, 2004. 
	.	[11]  HideyaOchiaiandHiroshiEsaki.Mobilityentropyandmessagerouting in community-structured delay tolerant networks. In Proceedings of the 4th Asian Conference on Internet Engineering, pages 93–102, 2008. 
	.	[12]  Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid data with reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications, pages 1698–1707. IEEE, 2020. 
	.	[13]  Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995. 
	.	[14]  Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. 
	.	[15]  Sepp Hochreiter and Ju ̈rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. 
[16] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimen- 
sionality of data with neural networks. science, 313(5786):504–507, 2006. 
[17] 
[18] [19] 
[20] [21] 
[22] 
[23] [24] 
[25] 
[26] 
[27] 
[28] 
[29] [30] 
[31] 
[32] [33] 
[34] [35] [36] 
[37] [38] 
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928–1937. PMLR, 2016. 
Magnus Frodigh, Per Johansson, and Peter Larsson. Wireless ad hoc networking: the art of networking without a network. Ericsson review, 4(4):249, 2000.Ru ̈diger Schollmeier. A definition of peer-to-peer networking for the classification of peer-to-peer architectures and applications. In Proceedings First International Conference on Peer-to-Peer Computing, pages 101–102. IEEE, 2001. 
Mehran Abolhasan, Tadeusz Wysocki, and Eryk Dutkiewicz. A review of routing protocols for mobile ad hoc networks. Ad hoc networks, 2(1):1–22, 2004. HannesHartensteinandLPLaberteaux.Atutorialsurveyonvehicularad hoc networks. IEEE Communications magazine, 46(6):164–171, 2008. Kevin Fall. A delay-tolerant network architecture for challenged Internets. In Proceedings of the 2003 conference on Applications, technologies, architectures, and protocols for computer communications, pages 27–34, 2003. 
Dinh C Nguyen, Ming Ding, Pubudu N Pathirana, Aruna Seneviratne, Jun Li, and H Vincent Poor. Federated learning for internet of things: A comprehensive survey. IEEE Communications Surveys & Tutorials, 2021. 
Abhijit Guha Roy, Shayan Siddiqui, Sebastian Po ̈lsterl, Nassir Navab, and Christian Wachinger. BrainTorrent: A peer-to-peer environment for decentralized federated learning. arXiv preprint arXiv:1905.06731, 2019. 
Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang Yan. A blockchain-based decentralized federated learning framework with committee consensus. IEEE Network, 35(1):234–241, 2020. 
Xianglin Bao, Cheng Su, Yan Xiong, Wenchao Huang, and Yifei Hu. FLchain: A blockchain for auditable federated learning with trust and incentive. In 2019 5th International Conference on Big Data Computing and Communications (BIGCOM), pages 151–159. IEEE, 2019. 
Shiva Raj Pokhrel and Jinho Choi. Federated learning with blockchain for autonomous vehicles: Analysis and design challenges. IEEE Trans- actions on Communications, 68(8):4734–4746, 2020.Muhammad Habib ur Rehman, Khaled Salah, Ernesto Damiani, and Davor Svetinovic. Towards blockchain-based reputation-aware federated learning. In IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS), pages 183–188. IEEE, 2020. 
Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingada- halli Shastry, Sathyanarayanan Manamohan, et al. Swarm learning for decentralized and confidential clinical machine learning. Nature, 594(7862):265–270, 2021. ChristodoulosPappas,DimitrisChatzopoulos,SpyrosLalis,andManolis Vavalis. IPLS: A framework for decentralized federated learning. In 2021 IFIP Networking Conference, pages 1–6. IEEE, 2021. 
Chenghao Hu, Jingyan Jiang, and Zhi Wang. Decentralized fed- erated learning: A segmented gossip approach. arXiv preprint arXiv:1908.07782, 2019.Zygmunt J Haas, Joseph Y Halpern, and Li Li. Gossip-based ad hoc routing. IEEE/ACM Transactions on networking, 14(3):479–491, 2006. Amin Vahdat, David Becker, et al. Epidemic routing for partially connected ad hoc networks, 2000. 
Bouziane Brik, Adlen Ksentini, and Maha Bouaziz. Federated learning for UAVs-enabled wireless networks: Use cases, challenges, and open problems. IEEE Access, 8:53841–53849, 2020.Ilker Bekmezci, Ozgur Koray Sahingoz, and S ̧amil Temel. Flying ad- hoc networks (FANETs): A survey. Ad Hoc Networks, 11(3):1254–1270, 2013. 
Yi Liu, Jiangtian Nie, Xuandi Li, Syed Hassan Ahmed, Wei Yang Bryan Lim, and Chunyan Miao. Federated learning in the sky: Aerial-ground air quality sensing framework with UAV swarms. IEEE Internet of Things Journal, 8(12):9827–9837, 2020. 
H. OCHIAI et al.: WIRELESS AD HOC FEDERATED LEARNING: A FULLY DISTRIBUTED COOPERATIVE MACHINE LEARNING 13 
	.	[39]  Yuntao Wang, Zhou Su, Ning Zhang, and Abderrahim Benslimane. Learning in the air: Secure federated learning for UAV-assisted crowd- sensing. IEEE Transactions on network science and engineering, 8(2):1055–1069, 2020. 
	.	[40]  Quoc-Viet Pham, Ming Zeng, Rukhsana Ruby, Thien Huynh-The, and Won-Joo Hwang. UAV communications for sustainable federated learning. IEEE Transactions on Vehicular Technology, 70(4):3944–3948, 2021. 
	.	[41]  Hamid Shiri, Jihong Park, and Mehdi Bennis. Communication-efficient massive UAV online path control: Federated learning meets mean-field game theory. IEEE Transactions on Communications, 68(11):6840– 6857, 2020. 
	.	[42]  Tracy Camp, Jeff Boleng, and Vanessa Davies. A survey of mobility models for ad hoc network research. Wireless communications and mobile computing, 2(5):483–502, 2002. 
	.	[43]  Juri Opitz and Sebastian Burst. Macro f1 and macro f1. arXiv preprint arXiv:1911.03347, 2019. 
	.	[44]  Leland McInnes, John Healy, and James Melville. Umap: uniform manifold approximation and projection for dimension reduction. 2020. 
	.	[45]  Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. SSD: Single shot multibox detector. In European conference on computer vision, pages 21–37. Springer, 2016. 
	.	[46]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 
	.	[47]  Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. MDETR-modulated detection for end- to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780–1790, 2021. 
	.	[48]  Nadeem Ahmed, Regio A Michelin, Wanli Xue, Sushmita Ruj, Robert Malaney, Salil S Kanhere, Aruna Seneviratne, Wen Hu, Helge Janicke, and Sanjay K Jha. A survey of COVID-19 contact tracing apps. IEEE Access, 8:134577–134601, 2020. 
	.	[49]  Karen Simonyan and Andrew Zisserman. Very deep convolu- tional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 
	.	[50]  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 
	.	[51]  Shukor Abd Razak, Normalia Samian, and Mohd Aizaini Maarof. A friend mechanism for mobile ad hoc networks. In 2008 The Fourth International Conference on Information Assurance and Security, pages 243–248. IEEE, 2008. 
	.	[52]  Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634–643. PMLR, 2019. 
	.	[53]  Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938–2948. PMLR, 2020. Hideya Ochiai (Member, IEEE) received B.E. and master of information science and technology from the University of Tokyo, Japan, in 2006 and 2008, respectively. He received Ph.D. of informa- tion science and technology from the University of Tokyo in 2011. He became an assistant professor, and associated professor from 2011, and 2017 of the University of Tokyo respectively. His research ranges from IoT system and protocol designs to peer-to-peer overlay networks, delay-disruption tol- erant networks, network security, and decentralized 
machine learning. He joined standardization activities of IEEE from 2008, and of ISO/IEC JTC1/SC6 from 2012. He is a chair of the board of the Green University of Tokyo Project from 2016, a chair of LAN-security monitoring project from 2018, a chair of decentralized AI project from 2022. 
Yuwei Sun (Member, IEEE) received the B.E. degree in computer science and technology from North China Electric Power University, in 2018, and the M.E. degree (Hons.) in information and communication engineering from the University of Tokyo, in 2021, where he is currently pursuing the Ph.D. degree with the Graduate School of Informa- tion Science and Technology. In 2020, he was the fellow of the Advanced Study Program (ASP) at the Massachusetts Institute of Technology. He has been working with the Campus Computing Centre, United 
Nations University Centre on Cybersecurity, since 2019. He is a member of the AI Security and Privacy Team with the RIKEN Center for Advanced Intelligence Project working on trustworthy AI, and a Research Fellow at the Japan Society for the Promotion of Science (JSPS). 
Qingzhe Jin is a Master course student in the Gradu- ate School of Information Science and Technology at the University of Tokyo. He received his B.E. degree in Computer Science and Technology from Peking University, China in 2020. His research interests in- clude but not limited to federated learning, computer networks and graph neural networks. 
Nattanon Wongwiwatchai is a Ph.D. candidate in the Graduate School of Information Science and Technology at the University of Tokyo. He received B.Eng. and M.Eng. in Computer Engineering from Chulalongkorn University, Thailand, in 2019 and 2021, respectively. His research interests are in mo- bile application security and privacy, and decentral- ized machine learning. 
Hiroshi Esaki (Member, IEEE) received Ph.D. from the University of Tokyo, Japan, in 1998. In 1987, he joined Research and Development Center, Toshiba Corporation. From 1990 to 1991, he was at Applied Research Laboratory of Bell-core Inc., New Jersey, as a residential researcher. From 1994 to 1996, he was at Center for Telecommunication Research of Columbia University in New York. From 1998, he has been serving as a professor at the University of Tokyo, and as a board member of WIDE Project. Currently, he is the executive director 
of IPv6 promotion council, vice president of JPNIC, IPv6 Forum Fellow, and director of WIDE Project, chief architect of Digital Agency, Ja